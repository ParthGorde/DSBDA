Steps for running Word count- Map-Reduce example-
1. format hadoop namenode

home/hduser: ~$ hadoop namenode -format
Note that hadoop namenode -format command should be executed once before we start
using Hadoop. If this command is executed again after Hadoop has been used, it'll destroy all the
data on the Hadoop file system.
1. Create directory on home WordCountAss – place WordCount.java to this directory
2. Create directory on home WordCountAss / input – to place a file ‘input.text’
3. Create directory on home WordCountAss / Wc_Classes – to place all class files of compiled
WordCount.java
2. start all dfs and yarn deomans
home/hduser: ~$ start-all.sh
home/hduser: ~$ jps
[ Make sure that all dfs and yarn deomans are running, if not stop all deomans and then remove
and recreate datanode and namenode diecrtories in hadoop space]
3. Set classpath
home/hduser: ~$ export HADOOP_CLASSPATH=$(hadoop classpath)
home/hduser: ~$ echo $HADOOP_CLASSPATH
4. Create directotries on hadoop file system
home/hduser: ~$ hadoop dfs -mkdir /wc
home/hduser: ~$ hadoop dfs -mkdir /wc /wc/input
5. Place input.txt on hadoop file system
home/hduser: ~$ hadoop fs -put '/home/hduser/WordCountAss/input/input.txt' /wc/input
6. Check on namenode web interface if file is copied on hadoop file system
7. Compile WordCount.java
home/hduser: ~$ cd WordCountAss
home/hduser/WordCountAss: ~$ javac -classpath
${HADOOP_CLASSPATH}-d'/home/hduser/WordCountAss/WC_classes'
'/home/hduser/WC/WordCount.java'
8. Create jar file
home/hduser/WordCountAss: ~$ jar cvf wc2.jar-C WC_classes/.
9. execute jar.
home/hduser/WordCountAss: ~$ hadoop jar /home/hduser/WordCountAss/wc2.jar
WordCount /wc/input /wc/output
10. Successful execution, observe the output at hadoop file system and copy the the same to view
on terminal.
home/hduser/WordCountAss: ~ $ hadoop dfs -cat /wc/output/*
11. stop all deamons
home/hduser: ~$ stop-all.sh


*****************


The provided code is a step-by-step guide for running a Word Count Map-Reduce example in Hadoop. Let's go through each step to understand what it does:

    Format the Hadoop NameNode:
    This step formats the Hadoop NameNode, which prepares it for use. It is a one-time command that initializes the file system metadata and directory structure.

    Create the necessary directories:
    Here, you create three directories: "WordCountAss" (the main directory), "WordCountAss/input" (to store the input file), and "WordCountAss/WC_Classes" (to store the compiled class files of the WordCount program).

    Start all DFS and YARN daemons:
    This step starts the DFS (Distributed File System) and YARN (Yet Another Resource Negotiator) daemons. These are essential components of Hadoop for distributed file storage and resource management, respectively.

    Set the classpath:
    The classpath is set to include the Hadoop libraries. This ensures that the Hadoop commands and classes are accessible during compilation and execution.

    Create directories on the Hadoop file system:
    Use the Hadoop command "hadoop dfs -mkdir" to create two directories: "/wc" and "/wc/input" on the Hadoop file system. These directories will be used to store the input and output files for the Word Count example.

    Place the input file on the Hadoop file system:
    The input file, "input.txt," is copied from the local file system to the Hadoop file system using the "hadoop fs -put" command. It is stored in the "/wc/input" directory.

    Check the Namenode web interface:
    This step suggests checking the Namenode web interface to verify if the input file has been successfully copied to the Hadoop file system. The Namenode web interface provides a graphical representation of the Hadoop cluster's status and file system.

    Compile WordCount.java:
    The WordCount.java file is compiled using the "javac" command. The "-classpath" option ensures that the Hadoop libraries are included during compilation. The compiled class files are placed in the "WordCountAss/WC_Classes" directory.

    Create a JAR file:
    A JAR (Java Archive) file is created using the "jar" command. It packages all the class files in the "WC_Classes" directory into a single JAR file named "wc2.jar."

    Execute the JAR file:
    The JAR file is executed using the "hadoop jar" command. The WordCount class is specified as the main class, followed by the input and output paths ("/wc/input" and "/wc/output" respectively).

    View the output:
    Once the execution is complete, the output can be observed on the Hadoop file system using the "hadoop dfs -cat" command. The output files are located in the "/wc/output" directory.

    Stop all daemons:
    Finally, all the Hadoop daemons are stopped using the "stop-all.sh" script. This ensures that the Hadoop cluster is cleanly shut down.

By following these steps, you can run the Word Count Map-Reduce example in Hadoop and obtain the word count output.


********* j code

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}